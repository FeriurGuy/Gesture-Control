# Hand Gesture Recognition System

A computer vision-based application that interprets hand gestures into spoken commands using real-time landmark detection. This system integrates **MediaPipe** for hand tracking and **Scikit-Learn** for pattern classification, featuring a touchless virtual interface and optimized audio feedback.

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![OpenCV](https://img.shields.io/badge/OpenCV-Computer%20Vision-green)
![License](https://img.shields.io/badge/License-MIT-grey)

## Table of Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Dependencies](#dependencies)
- [License](#license)

## Overview
This project aims to facilitate human-computer interaction (HCI) by converting static hand gestures into audio output. It provides an end-to-end pipeline for dataset collection, model training, and real-time inference. The system is designed to be lightweight and runs on standard CPU hardware without requiring heavy GPU acceleration.

## Key Features
* **Real-time Inference:** Utilizes MediaPipe Hands for low-latency landmark extraction (21 points).
* **Custom Dataset Generation:** Includes a dedicated tool for recording and labeling new gesture data.
* **Interactive Virtual UI:** Features a virtual button mechanism allowing users to control system parameters (e.g., audio toggle) via fingertip tracking.
* **Audio Caching System:** Implements an MD5-based caching mechanism to eliminate latency for repeated text-to-speech requests.
* **Robust Classification:** Uses a Random Forest Classifier with an 80/20 train-test split validation strategy.

## System Architecture
The application workflow consists of three primary modules:
1.  **Detection Module:** Captures video frames and extracts normalized hand landmark coordinates (x, y, z).
2.  **Prediction Engine:** Flattens the coordinate vector and feeds it into the trained machine learning model to predict the gesture class.
3.  **Feedback Loop:** Updates the visual overlay and triggers the audio engine if the prediction confidence and stability thresholds are met.

## Installation

### Prerequisites
* Python 3.8 or higher
* A functional webcam

### Setup Guide
1.  **Clone the repository**
    ```bash
    git clone [https://github.com/yourusername/gesture-recognition-system.git](https://github.com/yourusername/gesture-recognition-system.git)
    cd gesture-recognition-system
    ```

2.  **Create a Virtual Environment (Recommended)**
    ```bash
    python -m venv venv
    # Windows
    venv\Scripts\activate
    # macOS/Linux
    source venv/bin/activate
    ```

3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

## Usage

The system operates in three stages: Data Collection, Training, and Execution.

### 1. Data Collection
To add a new gesture to the system:
```bash
python collect_data.py