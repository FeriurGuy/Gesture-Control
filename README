# Smart Hand Gesture Recognition System

A computer vision application that translates real-time hand gestures into spoken audio commands. This system leverages **MediaPipe** for hand landmark detection and **Scikit-Learn** for gesture classification, featuring a virtual touchless interface and low-latency audio feedback.

![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)
![OpenCV](https://img.shields.io/badge/OpenCV-Computer%20Vision-green)
![License](https://img.shields.io/badge/license-MIT-grey)

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Key Features](#key-features)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Usage Workflow](#usage-workflow)
- [Project Structure](#project-structure)
- [License](#license)

## ğŸ” Overview
This project facilitates Human-Computer Interaction (HCI) by converting static hand gestures into audible speech. It provides an end-to-end pipeline allowing users to record custom datasets, train a machine learning model, and execute real-time inference using a standard webcam.

## âœ¨ Key Features
* **Real-time Detection:** High-performance hand tracking using MediaPipe (21 landmarks).
* **Virtual Interface:** "Touchless" virtual buttons controlled by fingertip coordinates.
* **Custom Data Collection:** Integrated tools to record and label new gesture datasets easily.
* **Audio Caching Engine:** Implements MD5 hashing to cache generated audio, eliminating network latency for repeated phrases.
* **Robust Classification:** Uses a Random Forest Classifier to ensure high accuracy with minimal computational load.

## ğŸ›  System Architecture
1.  **Input Layer:** Captures video feed and extracts hand landmarks (x, y, z coordinates).
2.  **Processing Layer:** Normalizes data and predicts the gesture class using the pre-trained model.
3.  **Interaction Layer:** Checks for virtual button collisions (e.g., Toggle Voice) and manages audio output queues.

## ğŸ’» Installation

### Prerequisites
* Python 3.8 or higher
* Webcam

### Steps
1.  **Clone the repository**
    ```bash
    git clone [https://github.com/FERI-RAMADHAN/smart-gesture-recognition.git](https://github.com/FERI-RAMADHAN/smart-gesture-recognition.git)
    cd smart-gesture-recognition
    ```

2.  **Create a Virtual Environment (Recommended)**
    ```bash
    # Windows
    python -m venv venv
    venv\Scripts\activate

    # macOS/Linux
    source venv/bin/activate
    ```

3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

## ğŸš€ Usage Workflow

This system is designed in three modular stages. Follow this order to get started:

### 1. Data Collection (`collect_data.py`)
Use this script to create a dataset for a new gesture.
```bash
python collect_data.py

```

* Enter the name of the gesture (e.g., "Hello").
* Press **'R'** to start recording (the system captures 150 samples).
* The data is saved automatically in the `data/` directory.

### 2. Model Training (`train_model.py`)

Run this script to train the AI model on your collected data.

```bash
python train_model.py

```

* The script loads all CSV files from `data/`.
* It trains a Random Forest Classifier.
* The trained model is saved to `models/gesture_model.pkl`.

### 3. Run Application (`main.py`)

Start the main interface for real-time recognition.

```bash
python main.py

```

* **Interact:** Use your index finger to "click" the virtual **VOICE** button to toggle sound.
* **Exit:** Press **'Q'** to close the application.

## ğŸ“‚ Project Structure

```text
â”œâ”€â”€ audio_cache/       # Auto-generated MP3 files for low-latency playback
â”œâ”€â”€ data/              # CSV datasets for training
â”œâ”€â”€ models/            # Serialized Machine Learning models (.pkl)
â”œâ”€â”€ collect_data.py    # Data acquisition tool
â”œâ”€â”€ main.py            # Main application entry point
â”œâ”€â”€ speaker.py         # Audio engine with caching logic
â”œâ”€â”€ train_model.py     # Model training script
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ README.md          # Project documentation

```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](https://www.google.com/search?q=LICENSE) file for details.

---

*Developed by Feri Ramadhan*
